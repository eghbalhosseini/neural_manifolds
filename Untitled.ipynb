{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__cuda available  False\n",
      "__Python VERSION: 3.6.10 (default, Jun  9 2020, 18:36:16) \n",
      "[GCC 8.3.0]\n",
      "__CUDNN VERSION: 7605\n",
      "__Number CUDA Devices: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "from torchvision import models\n",
    "from mftma.manifold_analysis_correlation import manifold_analysis_corr\n",
    "from mftma.utils.make_manifold_data import make_manifold_data\n",
    "from mftma.utils.activation_extractor import extractor\n",
    "from mftma.utils.analyze_pytorch import analyze\n",
    "import getpass\n",
    "import argparse\n",
    "from neural_manifold_utils import CFAR100_fake_dataset_mftma , save_dict\n",
    "from datetime import datetime\n",
    "print('__cuda available ',torch.cuda.is_available())\n",
    "print('__Python VERSION:', sys.version)\n",
    "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "import torch\n",
    "from torchvision import datasets, transforms, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ehoseini\n"
     ]
    }
   ],
   "source": [
    "user=getpass.getuser()\n",
    "print(user)\n",
    "if user=='eghbalhosseini':\n",
    "    save_dir='/Users/eghbalhosseini/MyData/neural_manifolds/network_training_on_synthetic/'\n",
    "    data_dir='/Users/eghbalhosseini/MyData/neural_manifolds/synthetic_datasets/'\n",
    "elif user=='ehoseini':\n",
    "    save_dir='/om/user/ehoseini/MyData/neural_manifolds/network_training_on_synthetic/'\n",
    "    data_dir='/om/user/ehoseini/MyData/neural_manifolds/synthetic_datasets/'\n",
    "datafile='synth_partition_nobj_50000_nclass_50_nfeat_3072_beta_0.01_sigma_1.50_norm_1.mat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"data\": shape (100000,), type \"<f8\"> is not a matlab type\n",
      "<HDF5 dataset \"ir\": shape (100000,), type \"<u8\"> is not a matlab type\n",
      "<HDF5 dataset \"jc\": shape (50051,), type \"<u8\"> is not a matlab type\n",
      "data type not supported: graph, uint32\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset = CFAR100_fake_dataset_mftma(data_dir=os.path.join(data_dir, datafile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_classes = 10\n",
    "examples_per_class = 10\n",
    "data = make_manifold_data(dataset, sampled_classes, examples_per_class, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path=save_dir+'VGG16_synthdata_'+dataset.structure+'_nclass_'+str(int(dataset.n_class))+'_n_exm_'+str(int(dataset.exm_per_class))\n",
    "model = models.vgg16(num_classes=dataset.n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['layer_0_Input',\n",
       " 'layer_1_Conv2d',\n",
       " 'layer_3_Conv2d',\n",
       " 'layer_6_Conv2d',\n",
       " 'layer_8_Conv2d',\n",
       " 'layer_11_Conv2d',\n",
       " 'layer_13_Conv2d',\n",
       " 'layer_15_Conv2d',\n",
       " 'layer_18_Conv2d',\n",
       " 'layer_20_Conv2d',\n",
       " 'layer_22_Conv2d',\n",
       " 'layer_25_Conv2d',\n",
       " 'layer_27_Conv2d',\n",
       " 'layer_29_Conv2d',\n",
       " 'layer_33_Linear',\n",
       " 'layer_36_Linear',\n",
       " 'layer_39_Linear']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "model = model.to(device)\n",
    "model = model.eval()\n",
    "activations = extractor(model, data, layer_types=['Conv2d', 'Linear'])\n",
    "list(activations.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projecting layer_1_Conv2d\n",
      "Projecting layer_3_Conv2d\n",
      "Projecting layer_6_Conv2d\n",
      "Projecting layer_8_Conv2d\n",
      "Projecting layer_11_Conv2d\n",
      "Projecting layer_13_Conv2d\n",
      "Projecting layer_15_Conv2d\n",
      "Projecting layer_18_Conv2d\n",
      "Projecting layer_20_Conv2d\n",
      "Projecting layer_22_Conv2d\n"
     ]
    }
   ],
   "source": [
    "for layer, data, in activations.items():\n",
    "    X = [d.reshape(d.shape[0], -1).T for d in data]\n",
    "    # Get the number of features in the flattened data\n",
    "    N = X[0].shape[0]\n",
    "    # If N is greater than 5000, do the random projection to 5000 features\n",
    "    if N > 5000:\n",
    "        print(\"Projecting {}\".format(layer))\n",
    "        M = np.random.randn(5000, N)\n",
    "        M /= np.sqrt(np.sum(M * M, axis=1, keepdims=True))\n",
    "        X = [np.matmul(M, d) for d in X]\n",
    "    activations[layer] = X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_0_Input capacity: 0.354178, radius 0.954466, dimension 4.889111, correlation 0.111076\n",
      "layer_1_Conv2d capacity: 0.347111, radius 0.973281, dimension 4.838531, correlation 0.111006\n",
      "layer_3_Conv2d capacity: 0.324501, radius 1.035510, dimension 5.094076, correlation 0.111028\n",
      "layer_6_Conv2d capacity: 0.318713, radius 1.027056, dimension 5.057843, correlation 0.111617\n",
      "layer_8_Conv2d capacity: 0.313656, radius 1.045578, dimension 5.108380, correlation 0.110979\n",
      "layer_11_Conv2d capacity: 0.330684, radius 1.010406, dimension 4.885698, correlation 0.127790\n",
      "layer_13_Conv2d capacity: 0.349242, radius 0.999653, dimension 4.779131, correlation 0.148460\n",
      "layer_15_Conv2d capacity: 0.334839, radius 1.010647, dimension 4.863199, correlation 0.160911\n",
      "layer_18_Conv2d capacity: 0.353067, radius 0.997954, dimension 4.670523, correlation 0.170372\n",
      "layer_20_Conv2d capacity: 0.382376, radius 0.944428, dimension 4.367276, correlation 0.150131\n",
      "layer_22_Conv2d capacity: 0.436850, radius 0.887231, dimension 3.925045, correlation 0.134555\n",
      "layer_25_Conv2d capacity: 0.520099, radius 0.868997, dimension 3.190682, correlation 0.163571\n",
      "layer_27_Conv2d capacity: 0.550153, radius 0.865652, dimension 2.958094, correlation 0.198856\n",
      "layer_29_Conv2d capacity: 0.590173, radius 0.913055, dimension 2.459411, correlation 0.210214\n",
      "layer_33_Linear capacity: 0.661146, radius 0.896878, dimension 2.009535, correlation 0.196254\n",
      "layer_36_Linear capacity: 0.517505, radius 1.154661, dimension 2.267136, correlation 0.192406\n",
      "layer_39_Linear capacity: 0.746878, radius 0.885795, dimension 1.604902, correlation 0.254747\n"
     ]
    }
   ],
   "source": [
    "capacities = []\n",
    "radii = []\n",
    "dimensions = []\n",
    "correlations = []\n",
    "for k, X, in activations.items():\n",
    "    # Analyze each layer's activations\n",
    "    a, r, d, r0, K = manifold_analysis_corr(X, 0, 300, n_reps=1)\n",
    "    # Compute the mean values\n",
    "    a = 1 / np.mean(1 / a)\n",
    "    r = np.mean(r)\n",
    "    d = np.mean(d)\n",
    "    print(\"{} capacity: {:4f}, radius {:4f}, dimension {:4f}, correlation {:4f}\".format(k, a, r, d, r0))\n",
    "\n",
    "    # Store for later\n",
    "    capacities.append(a)\n",
    "    radii.append(r)\n",
    "    dimensions.append(d)\n",
    "    correlations.append(r0)\n",
    "names = list(activations.keys())\n",
    "names = [n.split('_')[1] + ' ' + n.split('_')[2] for n in names]\n",
    "# save the results:\n",
    "current_time = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "results_file = os.path.join(save_dir,'mftma_'+model_save_path+'_'+current_time)\n",
    "data_ = {'capacities': capacities,\n",
    "             'radii': radii,\n",
    "             'dimensions': dimensions,\n",
    "             'correlations': correlations,\n",
    "             'names': names,\n",
    "             'analyze_exm_per_class': examples_per_class,\n",
    "             'analyze_n_class': sampled_classes\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-b0185e6e8c5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult_save_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'mftma_VGG16_synthdata_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_nclass_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_n_exm_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexm_per_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcurrent_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msave_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_save_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "result_save_path=save_dir+'mftma_VGG16_synthdata_'+train_dataset.structure+'_nclass_'+str(int(train_dataset.n_class))+'_n_exm_'+str(int(train_dataset.exm_per_class))+'_'+current_time\n",
    "save_dict(data_, result_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_save_path=save_dir+'mftma_VGG16_synthdata_'+train_dataset.structure+'_nclass_'+str(int(train_dataset.n_class))+'_n_exm_'+str(int(train_dataset.exm_per_class))+'_'+current_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = (0.5070751592371323, 0.48654887331495095, 0.4409178433670343)\n",
    "std = (0.2673342858792401, 0.2564384629170883, 0.27615047132568404)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR100('../data', train=True, download=True,\n",
    "                   transform=transform_train)\n",
    "test_dataset = datasets.CIFAR100('../data', train=False, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize(mean, std)\n",
    "                   ]))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
