{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import pickle\n",
    "import fnmatch\n",
    "from tqdm import tqdm\n",
    "from utils import save_dir, analyze_dir, results_dir,train_pool,save_dict\n",
    "\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_identifier = 'NN-tree_nclass=64_nobj=64000_nhier=6_beta=0.0923671_sigma=5.0_nfeat=936-train_test-fixed'\n",
    "model_identifier = 'linear_NN-tree_nclass=64_nobj=64000_nhier=6_beta=0.000161_sigma=5.0_nfeat=936-train_test-fixed'\n",
    "train_identifier = 'epochs-10_batch-32_lr-0.01_momentum-0.5_init-gaussian_std-1e-06'\n",
    "analyze_identifier='mftma-exm_per_class=50-proj=False-rand=True-kappa=1e-08-n_t=300-n_rep=5'\n",
    "#epochs-10_batch-32_lr-0.001_momentum-0.5_init-gaussian_std-0.0001\n",
    "#                           epochs-10_batch-32_lr-0.002_momentum-0.6_init-gaussian_std-1e-05 \\\n",
    "#                           epochs-10_batch-32_lr-0.01_momentum-0.5_init-gaussian_std-1e-06 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_files = []\n",
    "for file in os.listdir(os.path.join(save_dir, model_identifier, train_identifier)):\n",
    "    if fnmatch.fnmatch(file, '*.pth'):\n",
    "        training_files.append(os.path.join(save_dir, model_identifier, train_identifier, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_pkl_files = []\n",
    "for file in os.listdir(os.path.join(save_dir, analyze_identifier, model_identifier, train_identifier)):\n",
    "    if fnmatch.fnmatch(file, '*gradient_data_v3.pkl'):\n",
    "        grad_pkl_files.append(os.path.join(save_dir, analyze_identifier, model_identifier, train_identifier, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load extracted gradient data (from extraction script!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = [re.findall('/\\d+', x) for x in grad_pkl_files]\n",
    "s = [item for sublist in s for item in sublist]\n",
    "dummy_id = [(x.split('/')) for x in s]\n",
    "file_id = [int(x.split('/')[1]) for x in s]\n",
    "sorted_files = [grad_pkl_files[x] for x in np.argsort(file_id)]\n",
    "grad_pkl_files = sorted_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:ERROR: not a MATLAB datatype: <HDF5 dataset \"data\": shape (128252,), type \"|u1\">, (uint8)\n",
      "ERROR:root:ERROR: not a MATLAB datatype: <HDF5 dataset \"ir\": shape (128252,), type \"<u8\">, (uint64)\n",
      "ERROR:root:ERROR: not a MATLAB datatype: <HDF5 dataset \"jc\": shape (64128,), type \"<u8\">, (uint64)\n",
      "ERROR:root:ERROR: MATLAB type not supported: graph, (uint32)\n",
      "ERROR:root:ERROR: MATLAB type not supported: graph, (uint32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['layer_1_Linear', 'layer_2_Linear', 'layer_3_Linear']\n"
     ]
    }
   ],
   "source": [
    "params = train_pool[model_identifier]()\n",
    "params.load_dataset()\n",
    "layer_names = params.get_layer_names()[1:]\n",
    "print(layer_names)\n",
    "transfo_mat = params.dataset.transformation_mats\n",
    "tiled_transfo_mat = [np.tile(x, (1, 50)).reshape(-1, x.shape[1]) for x in transfo_mat]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['layer_3_Linear']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_names = params.get_layer_names()[3:]\n",
    "layer_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1060it [03:26,  5.14it/s]\n"
     ]
    }
   ],
   "source": [
    "epoch_data=[]\n",
    "for idx, file in tqdm(enumerate(grad_pkl_files)):\n",
    "        g = pickle.load(open(file, 'rb'))\n",
    "        batch_=dict()\n",
    "        for key in g['results'][0].keys():\n",
    "            e = np.asarray(g['results'][0][key]).squeeze()\n",
    "            batch_[key]=(np.reshape(e, [-1, e.shape[2]]))\n",
    "        epoch_data.append(batch_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_e_f_list=dict()\n",
    "for layer in layer_names:\n",
    "    e_f_=[x[layer] for x in epoch_data]\n",
    "    all_e_f_list[layer]=e_f_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzing layer_3_Linear \n",
      "\n",
      "analyzing hierarchy 0 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1060it [24:13,  1.37s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzing hierarchy 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1060it [11:52,  1.49it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzing hierarchy 2 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1060it [05:54,  2.99it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzing hierarchy 3 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1060it [03:05,  5.73it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzing hierarchy 4 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1060it [01:31, 11.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzing hierarchy 5 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1060it [00:46, 22.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved /mindhive/evlab/u/Shared/Greta_Eghbal_manifolds/extracted/mftma-exm_per_class=50-proj=False-rand=True-kappa=1e-08-n_t=300-n_rep=5/linear_NN-tree_nclass=64_nobj=64000_nhier=6_beta=0.000161_sigma=5.0_nfeat=936-train_test-fixed/epochs-10_batch-32_lr-0.01_momentum-0.5_init-gaussian_std-1e-06/linear_NN-tree_nclass=64_nobj=64000_nhier=6_beta=0.000161_sigma=5.0_nfeat=936-train_test-fixed_layer_3_Linear_gradient_pooled_v3_large.pkl\n"
     ]
    }
   ],
   "source": [
    "layer_gradient_dict=dict()\n",
    "for layer in layer_names:\n",
    "    print(f\"analyzing {layer} \\n\")\n",
    "    layer_branch_data=dict()\n",
    "    all_grad_data = []\n",
    "    e_f_list = all_e_f_list[layer]\n",
    "\n",
    "    for hier_id,transfo in enumerate(tiled_transfo_mat):\n",
    "        print(f\"analyzing hierarchy {hier_id} \\n\")\n",
    "        # get grad data:\n",
    "        # create combination pairs\n",
    "        example_per_class=int(np.unique(transfo.sum(axis=0)))\n",
    "        a = list(range(example_per_class))\n",
    "        combs = list(itertools.combinations_with_replacement(a, r=2))\n",
    "        combs_1 = list(itertools.combinations(a, r=2))\n",
    "        combs_1 = [(x[1], x[0]) for x in combs_1] #flip\n",
    "        all_combs = [combs, combs_1]\n",
    "        all_combs = [item for sublist in all_combs for item in sublist]\n",
    "        #if len(all_combs)>example_per_class**2:\n",
    "        select_combs = [all_combs[x] for x in np.random.choice(np.arange(len(all_combs)), size=1500)]\n",
    "        #else:\n",
    "        #    select_combs=all_combs\n",
    "        # use the combination to compute the differences in vectors\n",
    "        all_branch_diffs = []\n",
    "        for _, time_point in tqdm(enumerate(e_f_list)):\n",
    "            branch_diffs = []\n",
    "            for branch in transfo.transpose():  # for each single \"over\" branch, iterate through the categories\n",
    "                P12 = time_point[np.where(branch == 1), :].squeeze()  # extract each category\n",
    "                p12_diff = [np.diff(P12[x, :], axis=0) for x in\n",
    "                            select_combs]  # find all pairwise differences between all samples in cat 1 and cat 2 (the leaf branches)\n",
    "                branch_diffs.append(\n",
    "                    np.mean(np.stack(p12_diff).squeeze(), axis=0))  # take mean over all the pairwise diffs\n",
    "            all_branch_diffs.append(branch_diffs)\n",
    "        # compute the norms for differences\n",
    "        branch_norm = [[np.linalg.norm(x) for x in branch_diffs] for branch_diffs in all_branch_diffs]\n",
    "        branch_norm_mat = np.stack(branch_norm)\n",
    "        layer_branch_data[f\"hier_{hier_id}\"]=branch_norm_mat\n",
    "    layer_gradient_dict[layer]=layer_branch_data\n",
    "    # save layer data independently\n",
    "    layer_gradient_file = os.path.join(save_dir, analyze_identifier, model_identifier, train_identifier,\n",
    "                                 f'{model_identifier}_{layer}_gradient_pooled_v3_large.pkl')\n",
    "    d_layer = {'analyze_identifier': analyze_identifier,\n",
    "                'model_identifier': model_identifier,\n",
    "                'train_identifier': train_identifier,\n",
    "                'layer':layer,\n",
    "                'gradient_results': layer_branch_data}\n",
    "    save_dict(d_layer, layer_gradient_file)\n",
    "    print('saved ' + layer_gradient_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a transformation matrix that does not average across num examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
