{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__cuda available  True\n",
      "__Python VERSION: 3.6.10 (default, Apr 23 2020, 15:24:07) \n",
      "[GCC 8.3.0]\n",
      "__pyTorch VERSION: 1.5.0+cu101\n",
      "__CUDNN VERSION: 7603\n",
      "__Number CUDA Devices: 4\n",
      "__Device name: Quadro RTX 6000\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import os, sys\n",
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import socket\n",
    "from datetime import datetime\n",
    "import scipy.io as sio   \n",
    "from scipy.io import loadmat\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import getpass\n",
    "print('__cuda available ',torch.cuda.is_available())\n",
    "print('__Python VERSION:', sys.version)\n",
    "print('__pyTorch VERSION:', torch.__version__)\n",
    "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "print('__Device name:', torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ehoseini\n"
     ]
    }
   ],
   "source": [
    "user=getpass.getuser()\n",
    "print(user)\n",
    "if user=='eghbalhosseini':\n",
    "    save_dir='/Users/eghbalhosseini/MyData/neural_manifolds/CNN_training_on_parition/'\n",
    "    data_dir='/Users/eghbalhosseini/MyData/neural_manifolds/'\n",
    "elif user=='ehoseini':\n",
    "    save_dir='/om/user/ehoseini/MyData/neural_manifolds/CNN_training_on_parition/'\n",
    "    data_dir='/om/user/ehoseini/MyData/neural_manifolds/'\n",
    "#data_file='synthpartition_nobj_60000_nclass_10_nfeat_784_norm_1.mat'\n",
    "data_file='synthtree_nobj_100000_nclass_50_nfeat_784_norm_1.mat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (batch_idx % log_interval == 0) & (batch_idx!=0 & False):\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct = pred.eq(target.view_as(pred)).sum().item()\n",
    "            # \n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            n_iter=batch_idx+(epoch-1)*len(train_loader)\n",
    "            writer.add_scalar('Loss/train', loss,n_iter )\n",
    "            writer.add_scalar('Accuracy/train', 100. * correct / len(target), n_iter)\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad(): #don't save gradient\n",
    "        for data, target in test_loader:\n",
    "            if cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    n_iter=(epoch-1)\n",
    "    writer.add_scalar('Loss/test', test_loss,n_iter )\n",
    "    writer.add_scalar('Accuracy/test', 100. * correct / len(test_loader.dataset), n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creation of /om/user/ehoseini/MyData/neural_manifolds/CNN_training_on_parition/ failed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "current_time = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "\n",
    "access_rights = 0o755\n",
    "try:\n",
    "    os.mkdir(save_dir,access_rights)\n",
    "except OSError:\n",
    "    print('Creation of %s failed\\n'%save_dir)\n",
    "\n",
    "batch_size=32\n",
    "test_batch_size=200 \n",
    "train_set_size=60000\n",
    "test_set_size=1000\n",
    "epochs=100\n",
    "no_cuda=False\n",
    "save_epochs=False\n",
    "momentum=0.2\n",
    "lr=0.02\n",
    "seed=1\n",
    "gamma=0.7\n",
    "log_interval=250\n",
    "cuda = not no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "train_spec={'batch_size':batch_size,\n",
    "           'train_set_size':train_set_size,\n",
    "           'num_epochs':epochs,\n",
    "           'lr':lr,\n",
    "           'log_interval':log_interval,\n",
    "           'test_batch_size':test_batch_size\n",
    "           }\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class partition_dataset(Dataset):\n",
    "    def __init__(self, data_dir=data_dir):\n",
    "        self.data_dir=data_dir\n",
    "        self.dat , self.target=self.load_data()\n",
    "        self.n_samples=self.dat.shape[0]\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    def __getitem__(self, idx):\n",
    "        item=np.expand_dims(self.dat[idx],axis=0)\n",
    "        targ=np.squeeze(self.target[idx])\n",
    "        return torch.tensor(item,dtype=torch.float), torch.tensor(targ,dtype=torch.long)\n",
    "    def load_data(self):\n",
    "        annot=loadmat(self.data_dir) \n",
    "        ops_struct=annot['ops_out']\n",
    "        vals=ops_struct[0,0]\n",
    "        dat=vals['data']\n",
    "        self.adj=vals['Adjacency']\n",
    "        dat_new=dat[:,range(28*28)]\n",
    "        dat_new=np.reshape(dat_new,(-1,28,28))\n",
    "        target=np.double(np.transpose(vals['class_id'])-1.0)\n",
    "        return dat_new, target \n",
    "\n",
    "# load dataset \n",
    "partition_train_dataset=partition_dataset(data_dir=os.path.join(data_dir,data_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=partition_train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=partition_train_dataset,\n",
    "    batch_size=1000,\n",
    "    shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape torch.Size([1000, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "iteration=iter(test_loader)\n",
    "epoch_data,epoch_target = next(iteration)\n",
    "print('data shape',epoch_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the neural network, expand on top of nn.Module\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.fc = nn.Linear(7*7*32, 50)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "log_dir = os.path.join(save_dir,'runs', current_time + '_' + socket.gethostname())\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "model = CNN().to(device)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss();\n",
    "#optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "#writer.add_graph(model)\n",
    "writer.add_hparams(hparam_dict=train_spec,metric_dict={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [8000/100000 (8%)]\tLoss: 3.234474\n",
      "Train Epoch: 1 [16000/100000 (16%)]\tLoss: 2.257017\n",
      "Train Epoch: 1 [24000/100000 (24%)]\tLoss: 1.431118\n",
      "Train Epoch: 1 [32000/100000 (32%)]\tLoss: 0.987215\n",
      "Train Epoch: 1 [40000/100000 (40%)]\tLoss: 0.535908\n",
      "Train Epoch: 1 [48000/100000 (48%)]\tLoss: 0.333240\n",
      "Train Epoch: 1 [56000/100000 (56%)]\tLoss: 0.257507\n",
      "Train Epoch: 1 [64000/100000 (64%)]\tLoss: 0.188729\n",
      "Train Epoch: 1 [72000/100000 (72%)]\tLoss: 0.210575\n",
      "Train Epoch: 1 [80000/100000 (80%)]\tLoss: 0.132511\n",
      "Train Epoch: 1 [88000/100000 (88%)]\tLoss: 0.116477\n",
      "Train Epoch: 1 [96000/100000 (96%)]\tLoss: 0.075812\n",
      "\n",
      "Test set: Average loss: -2.9302, Accuracy: 99897/100000 (100%)\n",
      "\n",
      "Train Epoch: 2 [8000/100000 (8%)]\tLoss: 0.094199\n",
      "Train Epoch: 2 [16000/100000 (16%)]\tLoss: 0.060795\n",
      "Train Epoch: 2 [24000/100000 (24%)]\tLoss: 0.063859\n",
      "Train Epoch: 2 [32000/100000 (32%)]\tLoss: 0.067950\n",
      "Train Epoch: 2 [40000/100000 (40%)]\tLoss: 0.031534\n",
      "Train Epoch: 2 [48000/100000 (48%)]\tLoss: 0.029613\n",
      "Train Epoch: 2 [56000/100000 (56%)]\tLoss: 0.015452\n",
      "Train Epoch: 2 [64000/100000 (64%)]\tLoss: 0.029817\n",
      "Train Epoch: 2 [72000/100000 (72%)]\tLoss: 0.026821\n",
      "Train Epoch: 2 [80000/100000 (80%)]\tLoss: 0.010715\n",
      "Train Epoch: 2 [88000/100000 (88%)]\tLoss: 0.010708\n",
      "Train Epoch: 2 [96000/100000 (96%)]\tLoss: 0.015484\n",
      "\n",
      "Test set: Average loss: -3.9065, Accuracy: 99997/100000 (100%)\n",
      "\n",
      "Train Epoch: 3 [8000/100000 (8%)]\tLoss: 0.016071\n",
      "Train Epoch: 3 [16000/100000 (16%)]\tLoss: 0.005171\n",
      "Train Epoch: 3 [24000/100000 (24%)]\tLoss: 0.009581\n",
      "Train Epoch: 3 [32000/100000 (32%)]\tLoss: 0.003089\n",
      "Train Epoch: 3 [40000/100000 (40%)]\tLoss: 0.004134\n",
      "Train Epoch: 3 [48000/100000 (48%)]\tLoss: 0.012184\n",
      "Train Epoch: 3 [56000/100000 (56%)]\tLoss: 0.003666\n",
      "Train Epoch: 3 [64000/100000 (64%)]\tLoss: 0.003179\n",
      "Train Epoch: 3 [72000/100000 (72%)]\tLoss: 0.003313\n",
      "Train Epoch: 3 [80000/100000 (80%)]\tLoss: 0.009683\n",
      "Train Epoch: 3 [88000/100000 (88%)]\tLoss: 0.001615\n",
      "Train Epoch: 3 [96000/100000 (96%)]\tLoss: 0.000933\n",
      "\n",
      "Test set: Average loss: -4.4776, Accuracy: 100000/100000 (100%)\n",
      "\n",
      "Train Epoch: 4 [8000/100000 (8%)]\tLoss: 0.002258\n",
      "Train Epoch: 4 [16000/100000 (16%)]\tLoss: 0.001098\n",
      "Train Epoch: 4 [24000/100000 (24%)]\tLoss: 0.001378\n",
      "Train Epoch: 4 [32000/100000 (32%)]\tLoss: 0.002336\n",
      "Train Epoch: 4 [40000/100000 (40%)]\tLoss: 0.001727\n",
      "Train Epoch: 4 [48000/100000 (48%)]\tLoss: 0.001862\n",
      "Train Epoch: 4 [56000/100000 (56%)]\tLoss: 0.000854\n",
      "Train Epoch: 4 [64000/100000 (64%)]\tLoss: 0.000782\n",
      "Train Epoch: 4 [72000/100000 (72%)]\tLoss: 0.001955\n",
      "Train Epoch: 4 [80000/100000 (80%)]\tLoss: 0.000998\n",
      "Train Epoch: 4 [88000/100000 (88%)]\tLoss: 0.000672\n",
      "Train Epoch: 4 [96000/100000 (96%)]\tLoss: 0.000625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-a6211ae46d76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mepoch_dat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m#print('saved epoch '+str(epoch))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#scheduler.step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-eb5d95e60177>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mtest_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# sum up batch loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# get the index of the max log-probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_dat=train(epoch)\n",
    "    test()\n",
    "    #print('saved epoch '+str(epoch))\n",
    "    #scheduler.step()\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_train_data = torchvision.datasets.FashionMNIST('/om/user/ehoseini/MyData/mnist_data/',train=True,download=True,transform = transforms.Compose([\n",
    "        transforms.ToTensor()]))\n",
    "MNIST_test_data = torchvision.datasets.FashionMNIST('/om/user/ehoseini/MyData/mnist_data/',train=False,download=True,transform = transforms.Compose([\n",
    "        transforms.ToTensor()]))\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=MNIST_train_data,\n",
    "    batch_size=124,\n",
    "    shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=MNIST_test_data,\n",
    "    batch_size=1000,\n",
    "    shuffle=False)\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # define layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "\n",
    "    def forward(self, t):\n",
    "        # conv 1\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        # conv 2\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        # fc1\n",
    "        t = t.reshape(-1, 12*4*4)\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "        # fc2\n",
    "        t = self.fc2(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "        # output\n",
    "        t = self.out(t)\n",
    "        # don't need softmax here since we'll use cross-entropy as activation.\n",
    "\n",
    "        return t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
