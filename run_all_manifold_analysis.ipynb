{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__cuda available  False\n",
      "__Python VERSION: 3.7.7 (default, May  6 2020, 04:59:01) \n",
      "[Clang 4.0.1 (tags/RELEASE_401/final)]\n",
      "__CUDNN VERSION: None\n",
      "__Number CUDA Devices: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import pickle\n",
    "import os\n",
    "from torchvision import models\n",
    "from mftma.manifold_analysis_correlation import manifold_analysis_corr\n",
    "from mftma.utils.make_manifold_data import make_manifold_data\n",
    "from mftma.utils.activation_extractor import extractor\n",
    "from mftma.utils.analyze_pytorch import analyze\n",
    "import getpass\n",
    "import argparse\n",
    "from neural_manifold_utils import CFAR100_fake_dataset_mftma , save_dict, sub_data\n",
    "from datetime import datetime\n",
    "print('__cuda available ',torch.cuda.is_available())\n",
    "print('__Python VERSION:', sys.version)\n",
    "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "#import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import datasets, transforms, models\n",
    "import copy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eghbalhosseini\n"
     ]
    }
   ],
   "source": [
    "user=getpass.getuser()\n",
    "print(user)\n",
    "if user=='eghbalhosseini':\n",
    "    save_dir='/Users/eghbalhosseini/MyData/neural_manifolds/network_training_on_synthetic/'\n",
    "    data_dir='/Users/eghbalhosseini/MyData/neural_manifolds/synthetic_datasets/'\n",
    "    \n",
    "elif user=='ehoseini':\n",
    "    save_dir='/om/user/ehoseini/MyData/neural_manifolds/network_training_on_synthetic/'\n",
    "    data_dir='/om/user/ehoseini/MyData/neural_manifolds/synthetic_datasets/'\n",
    "train_dir=os.path.join(save_dir,'train_VGG16_synthdata_tree_nclass_50_n_exm_1000')\n",
    "epoch_id=1\n",
    "layer_num=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile=os.path.join('/Users/eghbalhosseini/Desktop/','train_epoch_1')\n",
    "epoch_dat=pickle.load(open(datafile,'rb'))\n",
    "#activations_cell=epoch_dat['activations_cell']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_cell = epoch_dat['activations_cell']\n",
    "mfmta_data_ = {'mftma_results': [],\n",
    "                   'train_spec': epoch_dat['train_spec'],\n",
    "                   'train_accuracy': epoch_dat['train_accuracy'],\n",
    "                   'train_success': epoch_dat['train_success'],\n",
    "                   'epoch': epoch_dat['epoch'],\n",
    "                   'layer_num':layer_num\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3, 32, 32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((activations_cell[0])['layer_0_Input'][1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hier_layer_names=[list(activations.keys()) for activations in activations_cell]\n",
    "layer_n=[x[layer_num] for x in hier_layer_names]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['layer_0_Input', 'layer_0_Input', 'layer_0_Input']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hier_layer_names = [list(activations.keys()) for activations in activations_cell]\n",
    "layer_id = [x[layer_num] for x in hier_layer_names]\n",
    "layer_activ_cell = [{layer_id[idx]: x[layer_id[idx]]} for idx, x in enumerate(activations_cell)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3, 32, 32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((layer_activ_cell[0])['layer_0_Input'][1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hier_id, activ_hier in enumerate(layer_activ_cell):\n",
    "    for layer, data in activ_hier.items():\n",
    "        X = [d.reshape(d.shape[0], -1).T for d in data]\n",
    "        N = X[0].shape[0]\n",
    "        if N > 5000:\n",
    "            print(\"Projecting {}\".format(layer))\n",
    "            M = np.random.randn(5000, N)\n",
    "            M /= np.sqrt(np.sum(M * M, axis=1, keepdims=True))\n",
    "            X = [np.matmul(M, d) for d in X]\n",
    "        activ_hier[layer] = X\n",
    "    layer_activ_cell[hier_id] = activ_hier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3072, 100)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(layer_activ_cell[1]['layer_0_Input'][1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_act={'layer_0_Input':layer_activ_cell[0]['layer_0_Input']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_0_Input capacity: 0.101733, radius 0.753693, dimension 26.134985, correlation 0.042738\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'capacities' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-6741b25dc2eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Store for later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mcapacities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mradii\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mdimensions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'capacities' is not defined"
     ]
    }
   ],
   "source": [
    "for k, X, in test_act.items():\n",
    "    # Analyze each layer's activations\n",
    "    a, r, d, r0, K = manifold_analysis_corr(X, 0, 300, n_reps=1)\n",
    "    # Compute the mean values\n",
    "    a = 1 / np.mean(1 / a)\n",
    "    r = np.mean(r)\n",
    "    d = np.mean(d)\n",
    "    print(\"{} capacity: {:4f}, radius {:4f}, dimension {:4f}, correlation {:4f}\".format(k, a, r, d, r0))\n",
    "\n",
    "    # Store for later\n",
    "    capacities.append(a)\n",
    "    radii.append(r)\n",
    "    dimensions.append(d)\n",
    "    correlations.append(r0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_0_Input\n",
      "layer_0_Input capacity: 0.102004, radius 0.754869, dimension 26.162407, correlation 0.042738\n",
      "layer_0_Input\n",
      "layer_0_Input capacity: 0.048093, radius 0.982784, dimension 41.949863, correlation 0.112095\n",
      "layer_0_Input\n"
     ]
    }
   ],
   "source": [
    "mftmas_cell=[]\n",
    "for hier_id, activ_hier in enumerate(layer_activ_cell):\n",
    "    data_ = {'capacities': [],\n",
    "             'radii': [],\n",
    "             'dimensions': [],\n",
    "             'correlations':[],\n",
    "             'layer':[],\n",
    "            'n_hier_class':[],\n",
    "            'hierarchy':hier_id}\n",
    "    capacities = []\n",
    "    radii = []\n",
    "    dimensions = []\n",
    "    correlations = []\n",
    "    data_['layer']=layer_n[hier_id]\n",
    "    data_['n_hier_class']=len(activ_hier[layer_n[hier_id]])\n",
    "    for k, X, in activ_hier.items():\n",
    "        print(k)\n",
    "    # Analyze each layer's activations\n",
    "        try:\n",
    "            a, r, d, r0, K = manifold_analysis_corr(X, 0, 300, n_reps=1)\n",
    "\n",
    "            # Compute the mean values\n",
    "            a = 1/np.mean(1/a)\n",
    "            r = np.mean(r)\n",
    "            d = np.mean(d)\n",
    "            print(\"{} capacity: {:4f}, radius {:4f}, dimension {:4f}, correlation {:4f}\".format(k, a, r, d, r0))\n",
    "        except:\n",
    "            a=np.nan\n",
    "            r=np.nan\n",
    "            d=np.nan\n",
    "            r0=np.nan\n",
    "        # Store for later\n",
    "        capacities.append(a)\n",
    "        radii.append(r)\n",
    "        dimensions.append(d)\n",
    "        correlations.append(r0)\n",
    "    # combine the results \n",
    "    data_['capacities']=capacities\n",
    "    data_['radii']=radii\n",
    "    data_['dimensions']=dimensions\n",
    "    data_['correlations']=correlations\n",
    "    mftmas_cell.append(data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mftmas_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(activ_hier[layer_n[hier_id]])\n",
    "#int(np.unique([len(activ_hier[x]) for x in data_['layers']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hier_id, activ_hier in enumerate(activations_cell):\n",
    "    layer_names=list(activ_hier.keys())\n",
    "    for layer, data, in activ_hier.items():\n",
    "        X = [d.reshape(d.shape[0], -1).T for d in data]\n",
    "        # Get the number of features in the flattened data\n",
    "        N = X[0].shape[0]\n",
    "        # If N is greater than 5000, do the random projection to 5000 features\n",
    "        if N > 5000:\n",
    "            print(\"Projecting {}\".format(layer))\n",
    "        #    M = np.random.randn(5000, N)\n",
    "        #    M /= np.sqrt(np.sum(M*M, axis=1, keepdims=True))\n",
    "        #    X = [np.matmul(M, d) for d in X]\n",
    "        #activ_hier[layer] = X\n",
    "    #activations_cell[hier_id]=activ_hier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mftmas_cell=[]\n",
    "for hier_id, activ_hier in enumerate(activations_cell):\n",
    "    data_ = {'capacities': [],\n",
    "             'radii': [],\n",
    "             'dimensions': [],\n",
    "             'correlations':[],\n",
    "             'layers':[],\n",
    "            'n_class':[],\n",
    "            'hierarchy':hier_id}\n",
    "    capacities = []\n",
    "    radii = []\n",
    "    dimensions = []\n",
    "    correlations = []\n",
    "    data_['layers']=activ_hier.keys()\n",
    "    data_['hier_n_class']=int(np.unique([len(activ_hier[x]) for x in data_['layers']]))\n",
    "    for k, X, in activ_hier.items():\n",
    "    # Analyze each layer's activations\n",
    "        a, r, d, r0, K = manifold_analysis_corr(X, 0, 300, n_reps=1)\n",
    "    \n",
    "        # Compute the mean values\n",
    "        a = 1/np.mean(1/a)\n",
    "        r = np.mean(r)\n",
    "        d = np.mean(d)\n",
    "        print(\"{} capacity: {:4f}, radius {:4f}, dimension {:4f}, correlation {:4f}\".format(k, a, r, d, r0))\n",
    "        # Store for later\n",
    "        capacities.append(a)\n",
    "        radii.append(r)\n",
    "        dimensions.append(d)\n",
    "        correlations.append(r0)\n",
    "    # combine the results \n",
    "    data_['capacities']=capacities\n",
    "    data_['radii']=radii\n",
    "    data_['dimensions']=dimensions\n",
    "    data_['correlations']=correlations\n",
    "\n",
    "    mftmas_cell.append(data_)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfmta_data_ = {'mftma_results': mftmas_cell,\n",
    "             'train_spec': epoch_dat['train_spec'],\n",
    "             'train_accuracy': epoch_dat['train_accuracy'],\n",
    "             'train_success': epoch_dat['train_success'],\n",
    "             'epoch': epoch_dat['epoch']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[activations.keys() for activations in activations_cell]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names=list(activationenumerate))\n",
    "layer=layer_names[1];\n",
    "data=activations[layer]\n",
    "X = [d.reshape(d.shape[0], -1).T for d in data]\n",
    "N = X[0].shape[0]\n",
    "# If N is greater than 5000, do the random projection to 5000 features\n",
    "if N > 5000:\n",
    "    print(\"Projecting {}\".format(layer))\n",
    "    M = np.random.randn(5000, N)\n",
    "    M /= np.sqrt(np.sum(M * M, axis=1, keepdims=True))\n",
    "    X = [np.matmul(M, d) for d in X]\n",
    "data=X\n",
    "capacities = []\n",
    "radii = []\n",
    "dimensions = []\n",
    "correlations = []\n",
    "a, r, d, r0, K = manifold_analysis_corr(data, 0, 300, n_reps=1)\n",
    "a = 1 / np.mean(1 / a)\n",
    "r = np.mean(r)\n",
    "d = np.mean(d)\n",
    "print(\"{} capacity: {:4f}, radius {:4f}, dimension {:4f}, correlation {:4f}\".format(layer, a, r, d, r0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_save_path = save_dir + 'mftma_VGG16_synthdata_' + train_dataset.structure + '_nclass_' + str(\n",
    "        int(train_dataset.n_class)) + '_n_exm_' + str(int(train_dataset.exm_per_class)) + str(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"data\": shape (100000,), type \"<f8\"> is not a matlab type\n",
      "<HDF5 dataset \"ir\": shape (100000,), type \"<u8\"> is not a matlab type\n",
      "<HDF5 dataset \"jc\": shape (50051,), type \"<u8\"> is not a matlab type\n",
      "data type not supported: graph, uint32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['layer_0_Input',\n",
       " 'layer_1_Conv2d',\n",
       " 'layer_3_Conv2d',\n",
       " 'layer_6_Conv2d',\n",
       " 'layer_8_Conv2d',\n",
       " 'layer_11_Conv2d',\n",
       " 'layer_13_Conv2d',\n",
       " 'layer_15_Conv2d',\n",
       " 'layer_18_Conv2d',\n",
       " 'layer_20_Conv2d',\n",
       " 'layer_22_Conv2d',\n",
       " 'layer_25_Conv2d',\n",
       " 'layer_27_Conv2d',\n",
       " 'layer_29_Conv2d',\n",
       " 'layer_33_Linear',\n",
       " 'layer_36_Linear',\n",
       " 'layer_39_Linear']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datafile='synth_partition_nobj_50000_nclass_50_nfeat_3072_beta_0.01_sigma_1.50_norm_1.mat'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset = CFAR100_fake_dataset_mftma(data_dir=os.path.join(data_dir, datafile))\n",
    "sampled_classes = 50\n",
    "examples_per_class = 100\n",
    "data = make_manifold_data(dataset, sampled_classes, examples_per_class, seed=0)\n",
    "model_save_path=save_dir+'VGG16_synthdata_'+dataset.structure+'_nclass_'+str(int(dataset.n_class))+'_n_exm_'+str(int(dataset.exm_per_class))\n",
    "model = models.vgg16(num_classes=dataset.n_class)\n",
    "model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "model = model.to(device)\n",
    "model = model.eval()\n",
    "activations = extractor(model, data, layer_types=['Conv2d', 'Linear'])\n",
    "list(activations.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3, 32, 32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(activations['layer_0_Input'][1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projecting layer_1_Conv2d\n",
      "Projecting layer_3_Conv2d\n",
      "Projecting layer_6_Conv2d\n",
      "Projecting layer_8_Conv2d\n",
      "Projecting layer_11_Conv2d\n",
      "Projecting layer_13_Conv2d\n",
      "Projecting layer_15_Conv2d\n",
      "Projecting layer_18_Conv2d\n",
      "Projecting layer_20_Conv2d\n",
      "Projecting layer_22_Conv2d\n"
     ]
    }
   ],
   "source": [
    "for layer, data, in activations.items():\n",
    "    X = [d.reshape(d.shape[0], -1).T for d in data]\n",
    "    # Get the number of features in the flattened data\n",
    "    N = X[0].shape[0]\n",
    "    # If N is greater than 5000, do the random projection to 5000 features\n",
    "    if N > 5000:\n",
    "        print(\"Projecting {}\".format(layer))\n",
    "        M = np.random.randn(5000, N)\n",
    "        M /= np.sqrt(np.sum(M * M, axis=1, keepdims=True))\n",
    "        X = [np.matmul(M, d) for d in X]\n",
    "    activations[layer] = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(activations['layer_0_Input'][1]).shape\n",
    "test_act={'layer_0_Input':activations['layer_0_Input']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_0_Input capacity: 0.141250, radius 0.695877, dimension 20.883590, correlation 0.026355\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'capacities' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-6741b25dc2eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Store for later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mcapacities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mradii\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mdimensions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'capacities' is not defined"
     ]
    }
   ],
   "source": [
    "for k, X, in test_act.items():\n",
    "    # Analyze each layer's activations\n",
    "    a, r, d, r0, K = manifold_analysis_corr(X, 0, 300, n_reps=1)\n",
    "    # Compute the mean values\n",
    "    a = 1 / np.mean(1 / a)\n",
    "    r = np.mean(r)\n",
    "    d = np.mean(d)\n",
    "    print(\"{} capacity: {:4f}, radius {:4f}, dimension {:4f}, correlation {:4f}\".format(k, a, r, d, r0))\n",
    "\n",
    "    # Store for later\n",
    "    capacities.append(a)\n",
    "    radii.append(r)\n",
    "    dimensions.append(d)\n",
    "    correlations.append(r0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, X, in activations.items():\n",
    "    # Analyze each layer's activations\n",
    "    a, r, d, r0, K = manifold_analysis_corr(X, 0, 300, n_reps=1)\n",
    "    # Compute the mean values\n",
    "    a = 1 / np.mean(1 / a)\n",
    "    r = np.mean(r)\n",
    "    d = np.mean(d)\n",
    "    print(\"{} capacity: {:4f}, radius {:4f}, dimension {:4f}, correlation {:4f}\".format(k, a, r, d, r0))\n",
    "\n",
    "    # Store for later\n",
    "    capacities.append(a)\n",
    "    radii.append(r)\n",
    "    dimensions.append(d)\n",
    "    correlations.append(r0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile='synth_partition_nobj_50000_nclass_50_nfeat_3072_beta_0.01_sigma_1.50_norm_1.mat'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset = CFAR100_fake_dataset_mftma(data_dir=os.path.join(data_dir, datafile))\n",
    "sampled_classes = 50\n",
    "examples_per_class = 100\n",
    "data = make_manifold_data(dataset, sampled_classes, examples_per_class, seed=0)\n",
    "model_save_path=save_dir+'VGG16_synthdata_'+dataset.structure+'_nclass_'+str(int(dataset.n_class))+'_n_exm_'+str(int(dataset.exm_per_class))\n",
    "model = models.vgg16(num_classes=dataset.n_class)\n",
    "model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "model = model.to(device)\n",
    "model = model.eval()\n",
    "activations = extractor(model, data, layer_types=['Conv2d', 'Linear'])\n",
    "list(activations.keys())\n",
    "for layer, data, in activations.items():\n",
    "    X = [d.reshape(d.shape[0], -1).T for d in data]\n",
    "    # Get the number of features in the flattened data\n",
    "    N = X[0].shape[0]\n",
    "    # If N is greater than 5000, do the random projection to 5000 features\n",
    "    if N > 5000:\n",
    "        print(\"Projecting {}\".format(layer))\n",
    "        M = np.random.randn(5000, N)\n",
    "        M /= np.sqrt(np.sum(M * M, axis=1, keepdims=True))\n",
    "        X = [np.matmul(M, d) for d in X]\n",
    "    activations[layer] = X\n",
    "capacities = []\n",
    "radii = []\n",
    "dimensions = []\n",
    "correlations = []\n",
    "for k, X, in activations.items():\n",
    "    # Analyze each layer's activations\n",
    "    a, r, d, r0, K = manifold_analysis_corr(X, 0, 300, n_reps=1)\n",
    "    # Compute the mean values\n",
    "    a = 1 / np.mean(1 / a)\n",
    "    r = np.mean(r)\n",
    "    d = np.mean(d)\n",
    "    print(\"{} capacity: {:4f}, radius {:4f}, dimension {:4f}, correlation {:4f}\".format(k, a, r, d, r0))\n",
    "\n",
    "    # Store for later\n",
    "    capacities.append(a)\n",
    "    radii.append(r)\n",
    "    dimensions.append(d)\n",
    "    correlations.append(r0)\n",
    "names = list(activations.keys())\n",
    "names = [n.split('_')[1] + ' ' + n.split('_')[2] for n in names]\n",
    "# save the results:\n",
    "current_time = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "results_file = os.path.join(save_dir,'mftma_'+model_save_path+'_'+current_time)\n",
    "data_1 = {'capacities': capacities,\n",
    "             'radii': radii,\n",
    "             'dimensions': dimensions,\n",
    "             'correlations': correlations,\n",
    "             'names': names,\n",
    "             'analyze_exm_per_class': examples_per_class,\n",
    "             'analyze_n_class': sampled_classes\n",
    "             }\n",
    "\n",
    "result_save_path=save_dir+'mftma_VGG16_synthdata_'+dataset.structure+'_nclass_'+str(int(dataset.n_class))+'_n_exm_'+str(int(dataset.exm_per_class))+'_'+current_time\n",
    "save_dict(data_1, result_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capacities=data_1['capacities']\n",
    "radii=data_1['radii']\n",
    "dimensions=data_1['dimensions']\n",
    "correlations=data_1['correlations']\n",
    "names=data_1['names']\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 4))\n",
    "plt.suptitle('Manifold Analysis results for Parition dataset; classes:'+ str(data_1['analyze_n_class'])+', example per class:'+str(data_1['analyze_exm_per_class']),\n",
    "            fontsize=16,fontweight='bold')\n",
    "axes[0].plot(capacities, linewidth=5)\n",
    "axes[1].plot(radii, linewidth=5)\n",
    "axes[2].plot(dimensions, linewidth=5)\n",
    "axes[3].plot(correlations, linewidth=5)\n",
    "\n",
    "axes[0].set_ylabel(r'$\\alpha_M$', fontsize=18)\n",
    "axes[1].set_ylabel(r'$R_M$', fontsize=18)\n",
    "axes[2].set_ylabel(r'$D_M$', fontsize=18)\n",
    "axes[3].set_ylabel(r'$\\rho_{center}$', fontsize=18)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xticks([i for i, _ in enumerate(names)])\n",
    "    ax.set_xticklabels(names, rotation=90, fontsize=16)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "#plt.tight_layout()\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile='synth_tree_nobj_50000_nclass_50_nfeat_3072_beta_0.01_sigma_1.50_norm_1.mat'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset = CFAR100_fake_dataset_mftma(data_dir=os.path.join(data_dir, datafile))\n",
    "sampled_classes = 50\n",
    "examples_per_class = 100\n",
    "data = make_manifold_data(dataset, sampled_classes, examples_per_class, seed=0)\n",
    "model_save_path=save_dir+'VGG16_synthdata_'+dataset.structure+'_nclass_'+str(int(dataset.n_class))+'_n_exm_'+str(int(dataset.exm_per_class))\n",
    "model = models.vgg16(num_classes=dataset.n_class)\n",
    "model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "model = model.to(device)\n",
    "model = model.eval()\n",
    "activations = extractor(model, data, layer_types=['Conv2d', 'Linear'])\n",
    "list(activations.keys())\n",
    "for layer, data, in activations.items():\n",
    "    X = [d.reshape(d.shape[0], -1).T for d in data]\n",
    "    # Get the number of features in the flattened data\n",
    "    N = X[0].shape[0]\n",
    "    # If N is greater than 5000, do the random projection to 5000 features\n",
    "    if N > 5000:\n",
    "        print(\"Projecting {}\".format(layer))\n",
    "        M = np.random.randn(5000, N)\n",
    "        M /= np.sqrt(np.sum(M * M, axis=1, keepdims=True))\n",
    "        X = [np.matmul(M, d) for d in X]\n",
    "    activations[layer] = X\n",
    "capacities = []\n",
    "radii = []\n",
    "dimensions = []\n",
    "correlations = []\n",
    "for k, X, in activations.items():\n",
    "    # Analyze each layer's activations\n",
    "    a, r, d, r0, K = manifold_analysis_corr(X, 0, 300, n_reps=1)\n",
    "    # Compute the mean values\n",
    "    a = 1 / np.mean(1 / a)\n",
    "    r = np.mean(r)\n",
    "    d = np.mean(d)\n",
    "    print(\"{} capacity: {:4f}, radius {:4f}, dimension {:4f}, correlation {:4f}\".format(k, a, r, d, r0))\n",
    "\n",
    "    # Store for later\n",
    "    capacities.append(a)\n",
    "    radii.append(r)\n",
    "    dimensions.append(d)\n",
    "    correlations.append(r0)\n",
    "names = list(activations.keys())\n",
    "names = [n.split('_')[1] + ' ' + n.split('_')[2] for n in names]\n",
    "# save the results:\n",
    "current_time = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "results_file = os.path.join(save_dir,'mftma_'+model_save_path+'_'+current_time)\n",
    "data_2 = {'capacities': capacities,\n",
    "             'radii': radii,\n",
    "             'dimensions': dimensions,\n",
    "             'correlations': correlations,\n",
    "             'names': names,\n",
    "             'analyze_exm_per_class': examples_per_class,\n",
    "             'analyze_n_class': sampled_classes\n",
    "             }\n",
    "\n",
    "result_save_path=save_dir+'mftma_VGG16_synthdata_'+dataset.structure+'_nclass_'+str(int(dataset.n_class))+'_n_exm_'+str(int(dataset.exm_per_class))+'_'+current_time\n",
    "save_dict(data_2, result_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data_2\n",
    "capacities=data['capacities']\n",
    "radii=data['radii']\n",
    "dimensions=data['dimensions']\n",
    "correlations=data['correlations']\n",
    "names=data['names']\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 4))\n",
    "plt.suptitle('Manifold Analysis results for Tree dataset; classes:'+ str(data['analyze_n_class'])+', example per class:'+str(data['analyze_exm_per_class']),\n",
    "            fontsize=16,fontweight='bold')\n",
    "axes[0].plot(capacities, linewidth=5)\n",
    "axes[1].plot(radii, linewidth=5)\n",
    "axes[2].plot(dimensions, linewidth=5)\n",
    "axes[3].plot(correlations, linewidth=5)\n",
    "\n",
    "axes[0].set_ylabel(r'$\\alpha_M$', fontsize=18)\n",
    "axes[1].set_ylabel(r'$R_M$', fontsize=18)\n",
    "axes[2].set_ylabel(r'$D_M$', fontsize=18)\n",
    "axes[3].set_ylabel(r'$\\rho_{center}$', fontsize=18)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xticks([i for i, _ in enumerate(names)])\n",
    "    ax.set_xticklabels(names, rotation=90, fontsize=16)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "#plt.tight_layout()\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile='synth_partition_nobj_100000_nclass_100_nfeat_3072_beta_0.01_sigma_1.50_norm_1.mat'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset = CFAR100_fake_dataset_mftma(data_dir=os.path.join(data_dir, datafile))\n",
    "sampled_classes = 100\n",
    "examples_per_class = 200\n",
    "data = make_manifold_data(dataset, sampled_classes, examples_per_class, seed=0)\n",
    "model_save_path=save_dir+'VGG16_synthdata_'+dataset.structure+'_nclass_'+str(int(dataset.n_class))+'_n_exm_'+str(int(dataset.exm_per_class))\n",
    "model = models.vgg16(num_classes=dataset.n_class)\n",
    "model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "model = model.to(device)\n",
    "model = model.eval()\n",
    "activations = extractor(model, data, layer_types=['Conv2d', 'Linear'])\n",
    "list(activations.keys())\n",
    "for layer, data, in activations.items():\n",
    "    X = [d.reshape(d.shape[0], -1).T for d in data]\n",
    "    # Get the number of features in the flattened data\n",
    "    N = X[0].shape[0]\n",
    "    # If N is greater than 5000, do the random projection to 5000 features\n",
    "    if N > 5000:\n",
    "        print(\"Projecting {}\".format(layer))\n",
    "        M = np.random.randn(5000, N)\n",
    "        M /= np.sqrt(np.sum(M * M, axis=1, keepdims=True))\n",
    "        X = [np.matmul(M, d) for d in X]\n",
    "    activations[layer] = X\n",
    "capacities = []\n",
    "radii = []\n",
    "dimensions = []\n",
    "correlations = []\n",
    "for k, X, in activations.items():\n",
    "    # Analyze each layer's activations\n",
    "    a, r, d, r0, K = manifold_analysis_corr(X, 0, 300, n_reps=1)\n",
    "    # Compute the mean values\n",
    "    a = 1 / np.mean(1 / a)\n",
    "    r = np.mean(r)\n",
    "    d = np.mean(d)\n",
    "    print(\"{} capacity: {:4f}, radius {:4f}, dimension {:4f}, correlation {:4f}\".format(k, a, r, d, r0))\n",
    "\n",
    "    # Store for later\n",
    "    capacities.append(a)\n",
    "    radii.append(r)\n",
    "    dimensions.append(d)\n",
    "    correlations.append(r0)\n",
    "names = list(activations.keys())\n",
    "names = [n.split('_')[1] + ' ' + n.split('_')[2] for n in names]\n",
    "# save the results:\n",
    "current_time = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "results_file = os.path.join(save_dir,'mftma_'+model_save_path+'_'+current_time)\n",
    "data_3 = {'capacities': capacities,\n",
    "             'radii': radii,\n",
    "             'dimensions': dimensions,\n",
    "             'correlations': correlations,\n",
    "             'names': names,\n",
    "             'analyze_exm_per_class': examples_per_class,\n",
    "             'analyze_n_class': sampled_classes\n",
    "             }\n",
    "\n",
    "result_save_path=save_dir+'mftma_VGG16_synthdata_'+dataset.structure+'_nclass_'+str(int(dataset.n_class))+'_n_exm_'+str(int(dataset.exm_per_class))+'_'+current_time\n",
    "save_dict(data_3, result_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data_3\n",
    "capacities=data['capacities']\n",
    "radii=data['radii']\n",
    "dimensions=data['dimensions']\n",
    "correlations=data['correlations']\n",
    "names=data['names']\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 4))\n",
    "plt.suptitle('Manifold Analysis results for Parition dataset; classes:'+ str(data['analyze_n_class'])+', example per class:'+str(data['analyze_exm_per_class']),\n",
    "            fontsize=16,fontweight='bold')\n",
    "axes[0].plot(capacities, linewidth=5)\n",
    "axes[1].plot(radii, linewidth=5)\n",
    "axes[2].plot(dimensions, linewidth=5)\n",
    "axes[3].plot(correlations, linewidth=5)\n",
    "\n",
    "axes[0].set_ylabel(r'$\\alpha_M$', fontsize=18)\n",
    "axes[1].set_ylabel(r'$R_M$', fontsize=18)\n",
    "axes[2].set_ylabel(r'$D_M$', fontsize=18)\n",
    "axes[3].set_ylabel(r'$\\rho_{center}$', fontsize=18)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xticks([i for i, _ in enumerate(names)])\n",
    "    ax.set_xticklabels(names, rotation=90, fontsize=16)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "#plt.tight_layout()\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# forth : super class analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile='synth_tree_nobj_50000_nclass_50_nfeat_3072_beta_0.01_sigma_1.50_norm_1.mat'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset = CFAR100_fake_dataset_mftma(data_dir=os.path.join(data_dir, datafile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hierarchy=len(dataset.vals.hierarchical_class_ids)\n",
    "hier_classes=[x-1 for x in dataset.vals.hierarchical_class_ids]\n",
    "hier_n_class=[int(max(x)+1) for x in hier_classes]\n",
    "exm_per_class=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hier_dat_mtmfa=[]\n",
    "for idx, x in enumerate(hier_classes):\n",
    "        dat_mfmta=[]\n",
    "        dat_mfmta=copy.deepcopy(dataset)\n",
    "        dat_mtmfa.target=hier_classes[idx]\n",
    "        hier_dat_mtmfa.append(copy.deepcopy(dat_mtmfa));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hier_dat_mtmfa[3].target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_manifold_data([dataset.dat,dataset.target], 50, exm_per_class, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hier_sample_mtmfa= [make_manifold_data(x, hier_n_class[idx], exm_per_class, seed=0) for idx, x in enumerate(hier_dat_mtmfa)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path=save_dir+'VGG16_synthdata_'+dataset.structure+'_nclass_'+str(int(dataset.n_class))+'_n_exm_'+str(int(dataset.exm_per_class))\n",
    "model = models.vgg16(num_classes=dataset.n_class)\n",
    "model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "model = model.to(device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_cell = [extractor(model,x,layer_types=['Conv2d', 'Linear']) for x in hier_sample_mtmfa]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path=save_dir+'VGG16_synthdata_'+dataset.structure+'_nclass_'+str(int(dataset.n_class))+'_n_exm_'+str(int(dataset.exm_per_class))\n",
    "model = models.vgg16(num_classes=dataset.n_class)\n",
    "model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "model = model.to(device)\n",
    "model = model.eval()\n",
    "activations = extractor(model, data, layer_types=['Conv2d', 'Linear'])\n",
    "list(activations.keys())\n",
    "for layer, data, in activations.items():\n",
    "    X = [d.reshape(d.shape[0], -1).T for d in data]\n",
    "    # Get the number of features in the flattened data\n",
    "    N = X[0].shape[0]\n",
    "    # If N is greater than 5000, do the random projection to 5000 features\n",
    "    if N > 5000:\n",
    "        print(\"Projecting {}\".format(layer))\n",
    "        M = np.random.randn(5000, N)\n",
    "        M /= np.sqrt(np.sum(M * M, axis=1, keepdims=True))\n",
    "        X = [np.matmul(M, d) for d in X]\n",
    "    activations[layer] = X\n",
    "capacities = []\n",
    "radii = []\n",
    "dimensions = []\n",
    "correlations = []\n",
    "for k, X, in activations.items():\n",
    "    # Analyze each layer's activations\n",
    "    a, r, d, r0, K = manifold_analysis_corr(X, 0, 300, n_reps=1)\n",
    "    # Compute the mean values\n",
    "    a = 1 / np.mean(1 / a)\n",
    "    r = np.mean(r)\n",
    "    d = np.mean(d)\n",
    "    print(\"{} capacity: {:4f}, radius {:4f}, dimension {:4f}, correlation {:4f}\".format(k, a, r, d, r0))\n",
    "\n",
    "    # Store for later\n",
    "    capacities.append(a)\n",
    "    radii.append(r)\n",
    "    dimensions.append(d)\n",
    "    correlations.append(r0)\n",
    "names = list(activations.keys())\n",
    "names = [n.split('_')[1] + ' ' + n.split('_')[2] for n in names]\n",
    "# save the results:\n",
    "current_time = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "results_file = os.path.join(save_dir,'mftma_'+model_save_path+'_'+current_time)\n",
    "data_4 = {'capacities': capacities,\n",
    "             'radii': radii,\n",
    "             'dimensions': dimensions,\n",
    "             'correlations': correlations,\n",
    "             'names': names,\n",
    "             'analyze_exm_per_class': examples_per_class,\n",
    "             'analyze_n_class': sampled_classes\n",
    "             }\n",
    "\n",
    "result_save_path=save_dir+'mftma_VGG16_synthdata_'+dataset.structure+'_nclass_'+str(hier_n_class)+'_n_exm_'+str(int(dataset.exm_per_class))+'_'+current_time\n",
    "save_dict(data_2, result_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data_4\n",
    "capacities=data['capacities']\n",
    "radii=data['radii']\n",
    "dimensions=data['dimensions']\n",
    "correlations=data['correlations']\n",
    "names=data['names']\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 4))\n",
    "plt.suptitle('Manifold Analysis results for Parition dataset; classes:'+ str(data['analyze_n_class'])+', example per class:'+str(data['analyze_exm_per_class']),\n",
    "            fontsize=16,fontweight='bold')\n",
    "axes[0].plot(capacities, linewidth=5)\n",
    "axes[1].plot(radii, linewidth=5)\n",
    "axes[2].plot(dimensions, linewidth=5)\n",
    "axes[3].plot(correlations, linewidth=5)\n",
    "\n",
    "axes[0].set_ylabel(r'$\\alpha_M$', fontsize=18)\n",
    "axes[1].set_ylabel(r'$R_M$', fontsize=18)\n",
    "axes[2].set_ylabel(r'$D_M$', fontsize=18)\n",
    "axes[3].set_ylabel(r'$\\rho_{center}$', fontsize=18)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xticks([i for i, _ in enumerate(names)])\n",
    "    ax.set_xticklabels(names, rotation=90, fontsize=16)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "#plt.tight_layout()\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_manifold",
   "language": "python",
   "name": "python_manifold"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
