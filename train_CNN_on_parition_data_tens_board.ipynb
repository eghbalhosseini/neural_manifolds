{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import socket\n",
    "from datetime import datetime\n",
    "import scipy.io as sio   \n",
    "from scipy.io import loadmat\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (batch_idx % log_interval == 0) & (batch_idx!=0):\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct = pred.eq(target.view_as(pred)).sum().item()\n",
    "            # \n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            n_iter=batch_idx+(epoch-1)*len(train_loader)\n",
    "            writer.add_scalar('Loss/train', loss,n_iter )\n",
    "            writer.add_scalar('Accuracy/train', 100. * correct / len(target), n_iter)\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad(): #don't save gradient\n",
    "        for data, target in test_loader:\n",
    "            if cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    n_iter=(epoch-1)\n",
    "    writer.add_scalar('Loss/test', test_loss,n_iter )\n",
    "    writer.add_scalar('Accuracy/test', 100. * correct / len(test_loader.dataset), n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creation of /Users/eghbalhosseini/MyData/neural_manifolds/CNN_training_on_parition/ failed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "current_time = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "save_dir='/Users/eghbalhosseini/MyData/neural_manifolds/CNN_training_on_parition/'\n",
    "data_dir='/Users/eghbalhosseini/MyData/neural_manifolds/data'\n",
    "data_file='synthpartition_eh.mat'\n",
    "access_rights = 0o755\n",
    "try:\n",
    "    os.mkdir(save_dir,access_rights)\n",
    "except OSError:\n",
    "    print('Creation of %s failed\\n'%save_dir)\n",
    "\n",
    "batch_size=32\n",
    "test_batch_size=200 \n",
    "train_set_size=5000\n",
    "test_set_size=1000\n",
    "epochs=10\n",
    "no_cuda=False\n",
    "save_epochs=False\n",
    "momentum=0.7\n",
    "lr=1e-5\n",
    "seed=1\n",
    "gamma=0.7\n",
    "log_interval=150\n",
    "cuda = not no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "train_spec={'batch_size':batch_size,\n",
    "           'train_set_size':train_set_size,\n",
    "           'num_epochs':epochs,\n",
    "           'lr':lr,\n",
    "           'log_interval':log_interval,\n",
    "           'test_batch_size':test_batch_size\n",
    "           }\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class partition_dataset(Dataset):\n",
    "    def __init__(self, data_dir=data_dir):\n",
    "        self.data_dir=data_dir\n",
    "        self.dat , self.target=self.load_data()\n",
    "        self.n_samples=self.dat.shape[0]\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    def __getitem__(self, idx):\n",
    "        item=np.expand_dims(self.dat[idx],axis=0)\n",
    "        targ=np.squeeze(self.target[idx])\n",
    "        return torch.tensor(item,dtype=torch.float), torch.tensor(targ,dtype=torch.long)\n",
    "    def load_data(self):\n",
    "        annot=loadmat(self.data_dir) \n",
    "        dat=annot['data']\n",
    "        dat_new=dat[:,range(28*28)]\n",
    "        dat_new=np.reshape(dat_new,(-1,28,28))\n",
    "        target=np.double(np.transpose(annot['class_id'])-1.0)\n",
    "        return dat_new, target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_train_dataset=partition_dataset(data_dir=os.path.join(data_dir,data_file))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=partition_train_dataset,\n",
    "    batch_size=30,\n",
    "    shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=partition_train_dataset,\n",
    "    batch_size=15,\n",
    "    shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partition_train_dataset.dat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the neural network, expand on top of nn.Module\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.fc = nn.Linear(7*7*32, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "log_dir = os.path.join(save_dir,'runs', current_time + '_' + socket.gethostname())\n",
    "writer = SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "criterion = nn.CrossEntropyLoss();\n",
    "#optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "#writer.add_graph(model)\n",
    "writer.add_hparams(hparam_dict=train_spec,metric_dict={})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [4500/20000 (22%)]\tLoss: 2.336850\n",
      "Train Epoch: 1 [9000/20000 (45%)]\tLoss: 2.419742\n",
      "Train Epoch: 1 [13500/20000 (67%)]\tLoss: 2.389538\n",
      "Train Epoch: 1 [18000/20000 (90%)]\tLoss: 2.478434\n",
      "saved epoch 1\n",
      "Train Epoch: 2 [4500/20000 (22%)]\tLoss: 2.368909\n",
      "Train Epoch: 2 [9000/20000 (45%)]\tLoss: 2.514724\n",
      "Train Epoch: 2 [13500/20000 (67%)]\tLoss: 2.380872\n",
      "Train Epoch: 2 [18000/20000 (90%)]\tLoss: 2.274499\n",
      "saved epoch 2\n",
      "Train Epoch: 3 [4500/20000 (22%)]\tLoss: 2.291887\n",
      "Train Epoch: 3 [9000/20000 (45%)]\tLoss: 2.333862\n",
      "Train Epoch: 3 [13500/20000 (67%)]\tLoss: 2.391939\n",
      "Train Epoch: 3 [18000/20000 (90%)]\tLoss: 2.302044\n",
      "saved epoch 3\n",
      "Train Epoch: 4 [4500/20000 (22%)]\tLoss: 2.314200\n",
      "Train Epoch: 4 [9000/20000 (45%)]\tLoss: 2.300629\n",
      "Train Epoch: 4 [13500/20000 (67%)]\tLoss: 2.372555\n",
      "Train Epoch: 4 [18000/20000 (90%)]\tLoss: 2.303049\n",
      "saved epoch 4\n",
      "Train Epoch: 5 [4500/20000 (22%)]\tLoss: 2.378007\n",
      "Train Epoch: 5 [9000/20000 (45%)]\tLoss: 2.310233\n",
      "Train Epoch: 5 [13500/20000 (67%)]\tLoss: 2.460665\n",
      "Train Epoch: 5 [18000/20000 (90%)]\tLoss: 2.330244\n",
      "saved epoch 5\n",
      "Train Epoch: 6 [4500/20000 (22%)]\tLoss: 2.355127\n",
      "Train Epoch: 6 [9000/20000 (45%)]\tLoss: 2.284608\n",
      "Train Epoch: 6 [13500/20000 (67%)]\tLoss: 2.299459\n",
      "Train Epoch: 6 [18000/20000 (90%)]\tLoss: 2.275795\n",
      "saved epoch 6\n",
      "Train Epoch: 7 [4500/20000 (22%)]\tLoss: 2.310320\n",
      "Train Epoch: 7 [9000/20000 (45%)]\tLoss: 2.352323\n",
      "Train Epoch: 7 [13500/20000 (67%)]\tLoss: 2.247440\n",
      "Train Epoch: 7 [18000/20000 (90%)]\tLoss: 2.383078\n",
      "saved epoch 7\n",
      "Train Epoch: 8 [4500/20000 (22%)]\tLoss: 2.185508\n",
      "Train Epoch: 8 [9000/20000 (45%)]\tLoss: 2.411023\n",
      "Train Epoch: 8 [13500/20000 (67%)]\tLoss: 2.375033\n",
      "Train Epoch: 8 [18000/20000 (90%)]\tLoss: 2.353322\n",
      "saved epoch 8\n",
      "Train Epoch: 9 [4500/20000 (22%)]\tLoss: 2.166322\n",
      "Train Epoch: 9 [9000/20000 (45%)]\tLoss: 2.259933\n",
      "Train Epoch: 9 [13500/20000 (67%)]\tLoss: 2.235045\n",
      "Train Epoch: 9 [18000/20000 (90%)]\tLoss: 2.331117\n",
      "saved epoch 9\n",
      "Train Epoch: 10 [4500/20000 (22%)]\tLoss: 2.226767\n",
      "Train Epoch: 10 [9000/20000 (45%)]\tLoss: 2.279714\n",
      "Train Epoch: 10 [13500/20000 (67%)]\tLoss: 2.222981\n",
      "Train Epoch: 10 [18000/20000 (90%)]\tLoss: 2.327151\n",
      "saved epoch 10\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_dat=train(epoch)\n",
    "    #print('saved epoch '+str(epoch))\n",
    "    #scheduler.step()\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_dnn",
   "language": "python",
   "name": "dnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
